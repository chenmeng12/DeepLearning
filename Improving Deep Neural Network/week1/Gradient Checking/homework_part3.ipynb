{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gc_utils import *\n",
        "from testCases import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": [
        "def forward_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    implement J(theat) \u003d x*theat\n",
        "    :param x: \n",
        "    :param theat: \n",
        "    :return: \n",
        "    \"\"\"\n",
        "    \n",
        "    J \u003d theta * x\n",
        "    \n",
        "    return J"
      ],
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "8"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 14
        }
      ],
      "source": [
        "x, theta \u003d 2, 4\n",
        "J \u003d forward_propagation(x, theta)\n",
        "J"
      ],
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "def backwrad_propagation(x, theta):\n",
        "    \"\"\"\n",
        "    Compurte derivative of J with repect to theta \n",
        "    :param x: real number\n",
        "    :param theta: real number\n",
        "    :return: the gradient of the cost with respect to theta\n",
        "    \"\"\"\n",
        "    \n",
        "    dtheta \u003d x\n",
        "    \n",
        "    return dtheta"
      ],
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "data": {
            "text/plain": "2"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 16
        }
      ],
      "source": [
        "x, theta \u003d 2, 4\n",
        "dtheta \u003d backwrad_propagation(x, theta)\n",
        "dtheta "
      ],
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "source": "def forward_propagation_n(X, Y, parameters):\n    \"\"\"\n    \n    :param X: training set for m examples\n    :param Y: labels for m examples\n    :param parameters: python dictionary containing your parameters W1, b1, W2, b2, W3, b3\n    :return: cost\n    \"\"\"\n    m \u003d X.shape[1]\n    W1 \u003d parameters[\"W1\"]\n    b1 \u003d parameters[\"b1\"]\n    W2 \u003d parameters[\"W2\"]\n    b2 \u003d parameters[\"b2\"]\n    W3 \u003d parameters[\"W3\"]\n    b3 \u003d parameters[\"b3\"]\n    \n    Z1 \u003d np.dot(W1, X) + b1\n    A1 \u003d relu(Z1)\n    Z2 \u003d np.dot(W2, A1) + b2\n    A2 \u003d relu(Z2)\n    Z3 \u003d np.dot(W3, A2) + b3\n    A3 \u003d sigmoid(Z3)\n    \n    logprobs \u003d np.multiply(-np.log(A3), Y) + np.multiply(-np.log(1 -A3), 1-Y)\n    cost \u003d 1./m * np.sum(logprobs)\n    \n    cache \u003d (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return cost, cache",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "source": "def backward_propagation_n(X, Y, cache):\n    \"\"\"\n    implement the backward propagation\n    :param X: input data, (input size, 1)\n    :param Y: label\n    :param cache: output from forward_propagation_n()\n    :return: gradients--A dictionary with the gradients of the cost with respect to each parameters\n    \"\"\"\n    m \u003d X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) \u003d cache\n    \n    dZ3 \u003d A3 - Y\n    dW3 \u003d 1./m * np.dot(dZ3, A2.T)\n    db3 \u003d 1./m * np.sum(dZ3, axis\u003d1, keepdims\u003dTrue)\n    \n    dA2 \u003d np.dot(W3.T, dZ3)\n    dZ2 \u003d np.multiply(dA2,np.int64(A2 \u003e 0))\n    dW2 \u003d 1./m * np.dot(dZ2, A1.T)\n    db2 \u003d 1./m * np.sum(dZ2, axis\u003d1, keepdims\u003dTrue)\n    \n    dA1 \u003d np.dot(W2.T, dZ2)\n    dZ1 \u003d np.multiply(dA1, np.int64(A1 \u003e 0))\n    dW1 \u003d 1./m * np.dot(dZ1, X.T)\n    db1 \u003d 1./m * np.sum(dZ1, axis\u003d1, keepdims\u003dTrue)\n    \n    gradients \u003d {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": "def gradient_check_n(parameters, gradients, X, Y, epsilon\u003d1e-7):\n    \n    \"\"\"\n    Check if backward_propagation_n computes correctly the gradients of the cost output bu forward_propagation_n\n    :param parameters: nerv network parameters\n    :param gradients: \n    :param X: \n    :param Y: \n    :param epsilon: \n    :return: \n    \"\"\"\n    parameters_values, _\u003d dictionary_to_vector(parameters)\n    grad \u003d gradients_to_vector(gradients)\n    num_parameters \u003d parameters_values.shape[0]\n    J_plus \u003d np.zeros((num_parameters, 1))\n    J_minus \u003d np.zeros((num_parameters, 1))\n    gradapprox \u003d np.zeros((num_parameters, 1))\n    \n    for i in range(num_parameters):\n        thetaplus \u003d np.copy(parameters_values)                                      # Step 1\n        thetaplus[i][0] \u003d thetaplus[i][0] + epsilon                                # Step 2\n        J_plus[i], _ \u003d forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                  # Step 3\n        ### END CODE HERE ###\n\n        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output \u003d \"J_minus[i]\".\n        ### START CODE HERE ### (approx. 3 lines)\n        thetaminus \u003d np.copy(parameters_values)                                     # Step 1\n        thetaminus[i][0] \u003d thetaminus[i][0] - epsilon                            # Step 2        \n        J_minus[i], _ \u003d forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  # Step 3\n        ### END CODE HERE ###\n\n        # Compute gradapprox[i]\n        ### START CODE HERE ### (approx. 1 line)\n        gradapprox[i] \u003d (J_plus[i] - J_minus[i]) / (2.* epsilon)\n        ### END CODE HERE ###\n\n    # Compare gradapprox to backward propagation gradients by computing difference.\n    ### START CODE HERE ### (approx. 1 line)\n    numerator \u003d np.linalg.norm(grad - gradapprox)                                           # Step 1\u0027\n    denominator \u003d np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2\u0027\n    difference \u003d numerator / denominator                                          # Step 3\u0027\n    ### END CODE HERE ###\n\n    if difference \u003e 1e-7:\n        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference \u003d \" + str(difference) + \"\\033[0m\")\n    else:\n        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference \u003d \" + str(difference) + \"\\033[0m\")\n\n    return difference",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u001b[93mThere is a mistake in the backward propagation! difference \u003d 1.189041787877932e-07\u001b[0m\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "X, Y, parameters \u003d gradient_check_n_test_case()\n\ncost, cache \u003d forward_propagation_n(X, Y, parameters)\ngradients \u003d backward_propagation_n(X, Y, cache)\ndifference \u003d gradient_check_n(parameters, gradients, X, Y)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}