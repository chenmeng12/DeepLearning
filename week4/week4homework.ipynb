{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "                  \n",
    "    parameters = {\n",
    "        \"W1\":W1,\n",
    "        \"b1\":b1,\n",
    "        \"W2\":W2,\n",
    "        \"b2\":b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172],\n",
       "        [-0.01072969,  0.00865408, -0.02301539]]), 'b1': array([[0.],\n",
       "        [0.]]), 'W2': array([[ 0.01744812, -0.00761207]]), 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.00319039, -0.0024937 ,  0.01462108, -0.02060141, -0.00322417],\n",
      "       [-0.00384054,  0.01133769, -0.01099891, -0.00172428, -0.00877858],\n",
      "       [ 0.00042214,  0.00582815, -0.01100619,  0.01144724,  0.00901591],\n",
      "       [ 0.00502494,  0.00900856, -0.00683728, -0.0012289 , -0.00935769]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-0.00267888,  0.00530355, -0.00691661, -0.00396754],\n",
      "       [-0.00687173, -0.00845206, -0.00671246, -0.00012665],\n",
      "       [-0.0111731 ,  0.00234416,  0.01659802,  0.00742044]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.26295337 -1.23429987]]\n",
      "(array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]]))\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "Z, cache = linear_forward(A, W, b)\n",
    "print(Z)\n",
    "print(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A_pre, W, b, activation):\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_pre, W, b)\n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert(A.shape == (W.shape[0], A_pre.shape[1]))    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.96890023 0.11013289]]\n",
      "[[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_pre, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, cache = linear_forward_activation(A_pre, W, b, 'sigmoid')\n",
    "print(A)\n",
    "\n",
    "A, cache = linear_forward_activation(A_pre, W, b, 'relu')\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_pre = A\n",
    "        A, cache = linear_forward_activation(A_pre, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_forward_activation(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -np.sum(np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \n",
    "    grads ={}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads['dA' + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA2': array([[ 0.12913162, -0.44014127],\n",
       "        [-0.14175655,  0.48317296],\n",
       "        [ 0.01663708, -0.05670698]]),\n",
       " 'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]),\n",
       " 'db2': array([[0.15187861]]),\n",
       " 'dA1': array([[ 0.        ,  0.52257901],\n",
       "        [ 0.        , -0.3269206 ],\n",
       "        [ 0.        , -0.32070404],\n",
       "        [ 0.        , -0.74079187]]),\n",
       " 'dW1': array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]),\n",
       " 'db1': array([[-0.22007063],\n",
       "        [ 0.        ],\n",
       "        [-0.02835349]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.59562069, -0.09991781, -2.14584584,  1.82662008],\n",
       "        [-1.76569676, -0.80627147,  0.51115557, -1.18258802],\n",
       "        [-1.0535704 , -0.86128581,  0.68284052,  2.20374577]]),\n",
       " 'b1': array([[-0.04659241],\n",
       "        [-1.28888275],\n",
       "        [ 0.53405496]]),\n",
       " 'W2': array([[-0.55569196,  0.0354055 ,  1.32964895]]),\n",
       " 'b2': array([[-0.84610769]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearset'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Illegal interpolation string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-429120804642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_orig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"y = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\". It's a \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m\" picture.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2699\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2700\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[1;32m-> 2701\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2702\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2703\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1810\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5490\u001b[0m         im = mimage.AxesImage(self, cmap, norm, interpolation, origin, extent,\n\u001b[0;32m   5491\u001b[0m                               \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5492\u001b[1;33m                               resample=resample, **kwargs)\n\u001b[0m\u001b[0;32m   5493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5494\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ax, cmap, norm, interpolation, origin, extent, filternorm, filterrad, resample, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m             \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m         )\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ax, cmap, norm, interpolation, origin, filternorm, filterrad, resample, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_filternorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_filterrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_interpolation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\APP\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_interpolation\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_interpd_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Illegal interpolation string'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Illegal interpolation string"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 10\n",
    "plt.imshow(train_x_orig[0])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255\n",
    "\n",
    "print(train_x_flatten.shape)\n",
    "print(test_x_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 12288\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layer_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    (n_x, n_h, n_y) = layer_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation='sigmoid')\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "        \n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "        \n",
    "        grads[\"dW1\"] = dW1\n",
    "        grads[\"db1\"] = db1\n",
    "        grads[\"dW2\"] = dW2\n",
    "        grads[\"db2\"] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and i%100 == 0:\n",
    "            print(\"Cost after iteration{}:{}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i%100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"iteration(per tens)\")\n",
    "        plt.title(\"learning_rate=\"+str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration0:0.6930497356599888\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
